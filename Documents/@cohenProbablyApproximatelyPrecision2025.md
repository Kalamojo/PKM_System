---
aliases:
  - "Probably Approximately Precision and Recall Learning"
authors: "Lee Cohen, Yishay Mansour, Shay Moran, Han Shao"
year:
  2025
type: paper
tags:
  - academic
page(s): ""
---
> [!abstract]
> Precision and Recall are fundamental metrics in machine learning tasks where both accurate predictions and comprehensive coverage are essential, such as in multi-label learning, language generation, medical studies, and recommender systems. A key challenge in these settings is the prevalence of one-sided feedback, where only positive examples are observed during trainingâ€”e.g., in multi-label tasks like tagging people in Facebook photos, we may observe only a few tagged individuals, without knowing who else appears in the image. To address learning under such partial feedback, we introduce a Probably Approximately Correct (PAC) framework in which hypotheses are set functions that map each input to a set of items, extending beyond single-label predictions and generalizing classical binary, multi-class, and multi-label models. Our results reveal sharp statistical and algorithmic separations from standard settings: classical methods such as Empirical Risk Minimization provably fail, even for simple hypothesis classes. We develop new algorithms that learn from positive data alone, achieving optimal sample complexity in the realizable case, and establishing multiplicativeâ€”rather than additiveâ€”approximation guarantees in the agnostic case, where achieving additive regret is impossible.

# Annotations
(2/11/2026, 7:33:20 PM)

"lprecision(g) := Exâˆ¼D  |g(x) \ gtarget(x)|  |g(x)| and lrecall(g) := Exâˆ¼D  |gtarget(x) \ g(x)|  |gtarget(x)| ,  where D denotes the distribution over inputs. Precision and recall are equal to 1 minus the precision and recall losses, respectively." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 2](zotero://open-pdf/library/items/9ZBAH8MA?page=2&annotation=RW86L5ND))

"In fact, it is not just that ERM would fail; we provide an example in which two hypotheses have nearly the same recall loss but very different precision losses, making it impossible to determine which hypothesis has better precision loss based solely on the training data" ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 3](zotero://open-pdf/library/items/9ZBAH8MA?page=3&annotation=S2RGDJ9M))([Cohenprobablyapproximatelyprecision2025](zotero://open-pdf/library/items/9ZBAH8MA?page=3&annotation=S2RGDJ9M)) Recall also suffers here. An example can be constructed, where there are few ground truth answers, yet still, one of them were missed in the training data. A hypothesis that outputs this among many false answers, versus another hypothesis that outputs all false answers, would have nearly the same precision but very different recall losses.

"Realizable Setting: We design algorithms that, given a sample of size  O log(|H|/Î´)  Îµ,  achieve recall and precision losses of at most Îµ with probability at least 1 âˆ’ Î´" ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 3](zotero://open-pdf/library/items/9ZBAH8MA?page=3&annotation=9YG7BQR7))

"In the context of recommender systems, recommending a list of items has also been addressed in cascading bandits [KSWA15]. However, while our objective is to identify the items that each input likes, their focus is on learning the top K items that are liked by most inputs." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 4](zotero://open-pdf/library/items/9ZBAH8MA?page=4&annotation=4KFXKWIL))([Cohenprobablyapproximatelyprecision2025](zotero://open-pdf/library/items/9ZBAH8MA?page=4&annotation=4KFXKWIL)) A little confused by this. Might check out cascading bandits paper

"Our model is inherently more challenging than traditional multi-label learning because our training set consists of examples, each associated with only a single correct label rather than all possible correct labels, with no negative feedback. Yet, at test time, the learner still needs to predict a list of relevant labels for new examples." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 4](zotero://open-pdf/library/items/9ZBAH8MA?page=4&annotation=3IU4IREF))

"For any Î± > 0, we say Î±-approximate optimal scalar loss is achievable if there exists a polynomial P such that, for any finite hypothesis class H, there is an algorithm A such that the following holds: For any Îµ, Î´ > 0 and any distribution D, if A is given an IID training set of size P (log|H|, 1/Îµ, 1/Î´), with probability at least 1 âˆ’ Î´, it outputs a hypothesis with scalar loss satisfying  lscalar(goutput) â‰¤ Î± Â· min  gâˆˆH lscalar(g) + Îµ ." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 5](zotero://open-pdf/library/items/9ZBAH8MA?page=5&annotation=TFSTW4Y8))

"Pareto-Loss Objective Let p, pâ€², r, râ€² âˆˆ [0, 1]. We write (p, r) =â‡’ (pâ€², râ€²) to denote the following statement: there exists a polynomial P such that, for any finite hypothesis class H, there is an algorithm A such that the following holds: If D is a distribution for which there exists a hypothesis in H with precision and recall losses (p, r), then for any Îµ, Î´ > 0, if A is given p, r and an IID training set of size P (log|H|, 1/Îµ, 1/Î´), with probability at least 1 âˆ’ Î´, it outputs a hypothesis with precision and recall losses at most pâ€² + Îµ and râ€² + Îµ.4" ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 5](zotero://open-pdf/library/items/9ZBAH8MA?page=5&annotation=2IQMY4RJ))([Cohenprobablyapproximatelyprecision2025](zotero://open-pdf/library/items/9ZBAH8MA?page=5&annotation=2IQMY4RJ)) Somewhat unclear. Are we saying that given that there is a hypothesis that achieves p and r losses, we can guarantee with high probability that there is a (better?) hypothesis that achieves p' and r' losses (with some margin of error)?

"One might argue that the issue in the above example arises from the large size of the ground-truth set; however, we will later show that even when the ground-truth set has a small size, accurately estimating and optimizing precision remains impossible." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 6](zotero://open-pdf/library/items/9ZBAH8MA?page=6&annotation=L5DS2FQE))([Cohenprobablyapproximatelyprecision2025](zotero://open-pdf/library/items/9ZBAH8MA?page=6&annotation=L5DS2FQE)) I mean I would argue that recall suffers more in this case, but this still seems to hold true

"Any consistent hypothesis will have low empirical recall loss by applying standard concentration inequality and thus, lbrecall(goutput) is small" ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 7](zotero://open-pdf/library/items/9ZBAH8MA?page=7&annotation=PEWKEQFU))([Cohenprobablyapproximatelyprecision2025](zotero://open-pdf/library/items/9ZBAH8MA?page=7&annotation=PEWKEQFU)) Not familiar with how this can be confirmed mathematically

"The main technical challenge in the analysis is how to connect precision loss with likelihood." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 7](zotero://open-pdf/library/items/9ZBAH8MA?page=7&annotation=MUB4F9LA))

"For any hypothesis g, we define a vector vg : H Ã— H â†’ [0, 1]" ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 7](zotero://open-pdf/library/items/9ZBAH8MA?page=7&annotation=879GZG44))([Cohenprobablyapproximatelyprecision2025](zotero://open-pdf/library/items/9ZBAH8MA?page=7&annotation=879GZG44)) What makes v_g a vector here? Is this more of a matrix, where every hypothesis in the rows (all hypotheses) are paired against every hypothesis in the columns (all hypotheses again) and v_g(g_row, g_column) is computed for all? So a matrix of probabilities/fractions?

"vg(gâ€², gâ€²â€²) = 1  m  m  X  i=1  Pvâˆ¼Unif(g(xi)) v âˆˆ gâ€²(xi) \ gâ€²â€²(xi) .  Intuitively, vg(gâ€², gâ€²â€²) represents the fraction of correct labels that are output by gâ€² but not by gâ€²â€² in the counterfactual scenario where g is the target hypothesis." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 7](zotero://open-pdf/library/items/9ZBAH8MA?page=7&annotation=JYVM55PX))

"We then define a metric dH between two hypotheses g1 and g2 by  dH(g1, g2) = âˆ¥vg1 âˆ’ vg2 âˆ¥âˆž .  Surprisingly, we show that dH(gtarget, g) is a surrogate for the scalar loss, providing both lower and upper bounds on the scalar loss with a constant multiplicative factor." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 7](zotero://open-pdf/library/items/9ZBAH8MA?page=7&annotation=EP28JENG))

"there exists a hypothesis gâ€  âˆˆ H with lrecall(gâ€ ) = 1  4 and lprecision(gâ€ ) = 7  16 s.t. for any sample size m > 0, with probability 1 over the training set, the expected (over the randomness of the algorithm) precision and recall losses of the output goutput satisfy  E lrecall(goutput) + 12  5 E lprecision(goutput) â‰¥ 7  5.  Remark 1. Hence the output hypothesis either suffers lrecall(goutput) > 1  4 = lrecall(gâ€ ) or lprecision(goutput) â‰¥  23  48 = 23  21 lprecision(gâ€ ). Thus ( 7  16 , 1  4 ) Ì¸â‡’ ( 7  16 + 0.01, 1  4 + 0.01)." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 8](zotero://open-pdf/library/items/9ZBAH8MA?page=8&annotation=BVAU3JZK))

"In the Pareto-loss case, we ask a more general question: which pairs of guarantees (pâ€², râ€²) are achievable, given that there exists a hypothesis in the class with precision and recall losses (p, r)? Since the recall loss is optimizable, for any given r, if there exists a hypothesis in the class with recall loss r, we can always achieve that recall loss. Therefore, we refine our question as follows: given any p, r âˆˆ [0, 1], what is the minimum precision loss pâ€² such that (p, r) â‡’ (pâ€², r)?" ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 8](zotero://open-pdf/library/items/9ZBAH8MA?page=8&annotation=2LD6AZL4))([Cohenprobablyapproximatelyprecision2025](zotero://open-pdf/library/items/9ZBAH8MA?page=8&annotation=2LD6AZL4)) Essentially, imagine there exists a hypothesis with 50% recall and 50% precision, This is asking which sliders on precision and recall along the Pareto-frontier are achievable. For instance, by sacrificing recall to 25%, maybe 75% precision is guaranteed achievable (with some probability and +- some error margin)

"There exist an algorithm such that given an IID training set of size m â‰¥ O( log(|H|/Î´)  Îµ2 ),  with probability at least 1 âˆ’ Î´, the output hypothesis goutput satisfies  lscalar(goutput) â‰¤ 5 Â· min  gâˆˆH lscalar(g) + Îµ .  This implies that for any p, r âˆˆ [0, 1], (p, r) â‡’ (5(p + r), r) ." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 8](zotero://open-pdf/library/items/9ZBAH8MA?page=8&annotation=YCEUJMFH))

"in the agnostic setting, applying maximum likelihood directly no longer works. This is because it is possible that none of the hypotheses in the hypothesis class are consistent with the training set and thus all hypotheses have zero likelihood. Instead, we make some modifications to adapt the maximum likelihood idea work for Pareto-loss objective. As mentioned earlier, the maximum likelihood method is equivalent to returning the hypothesis with the minimum sum of log output sizes among all consistent hypotheses. Hence, it can be decomposed into two steps: minimizing the recall loss and then regulating by minimizing the sum of log output sizes." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 8](zotero://open-pdf/library/items/9ZBAH8MA?page=8&annotation=BEVPXZF5))

"We are therefore interested in the following question: whether (p = 0, r) â‡’ (pâ€² = 0, râ€² = r)? We propose an algorithm with sample complexity depending on output set size of the target hypothesis and show that it is impossible to achieve zero precision loss with sample complexity independent of the target hypothesisâ€™s output set size." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 9](zotero://open-pdf/library/items/9ZBAH8MA?page=9&annotation=7U2QPFVU))([Cohenprobablyapproximatelyprecision2025](zotero://open-pdf/library/items/9ZBAH8MA?page=9&annotation=7U2QPFVU)) Even if there exists some hypothesis with perfect precision, without knowing that the number of ground truth answers is small (or large), there's no way of really narrowing down hypotheses by the size of their outputs to mimic precision loss

"When the target hypothesisâ€™s output size ngtarget(x) is bounded by C almost everywhere, we have  E |g(x) âˆ© gtarget(x)|  ng(x) Â· ngtarget (x) = E 1 âˆ’ lprecision(g, x)  ngtarget (x) â‰¤ E 1  ngtarget (x) âˆ’ E lprecision(g, x)  C.  Therefore, we have âˆ†g,D â‰¥ lprecision(g)  C . This implies that when the target hypothesis has bounded output size, we are able to find the hypothesis with zero precision loss. However, when the target hypothesisâ€™s output size becomes too large, we show that it is impossible to achieve precision-recall of (0, r)." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 10](zotero://open-pdf/library/items/9ZBAH8MA?page=10&annotation=NFMX7TEE))([Cohenprobablyapproximatelyprecision2025](zotero://open-pdf/library/items/9ZBAH8MA?page=10&annotation=NFMX7TEE)) Makes perfect sense. If the ground truth answers size is large, then we cannot verifiably penalize or eliminate hypotheses for having large prediction sets. Suddenly, outputting very few items is not necessarily a good indicator that your answers are more precise

"In the realizable setting, the target hypothesis gtarget is in the hypothesis class. Given the IID training data (x1, v1), . . . , (xm, vm), the maximum likelihood method returns the hypothesis  goutput = arg max  gâˆˆH  m  Y  i=1  1(vi âˆˆ g(xi))  ng(xi) ." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 14](zotero://open-pdf/library/items/9ZBAH8MA?page=14&annotation=DLH6EBJF))

![[Attachments/ðŸ’¤Zotero/cohenProbablyApproximatelyPrecision2025/cohenProbablyApproximatelyPrecision2025-17-x67-y121.png]]

"Nevertheless, hypotheses with optimal F1 or FÎ² scores are specific points on this Pareto front, and it will be interesting for future work to find them efficiently." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 29](zotero://open-pdf/library/items/9ZBAH8MA?page=29&annotation=ZDH776ZR))

"Second, it remains an open question whether there exists a combinatorial measure, similar to the VC dimension in standard PAC learning, that characterizes the learnability of precision and recall." ([Cohenprobablyapproximatelyprecision2025](zotero://select/library/items/XTHEWPNU)) ([pdf, p. 30](zotero://open-pdf/library/items/9ZBAH8MA?page=30&annotation=YB3E6D6S))

## References
1. [cohenProbablyApproximatelyPrecision2025](zotero://select/library/items/XTHEWPNU)
