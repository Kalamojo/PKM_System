# 2-26 Discussion Questions/Topics

## PyTorch FSDP 2023

1. Is there a price paid for the separation of a model into partitions on different devices in terms of communication latency? How about with sharding?
2. Should Fully Sharded Data Parallel (FSDP) training always be preferred to Distributed Data Parallel (DDP) training?