---
aliases:
  - Learning to Answer from Correct Demonstrations
authors: Nirmit Joshi, Gene Li, Siddharth Bhandari, Shiva Prasad Kasiviswanathan, Cong Ma, Nathan Srebro
year: 2025
type: paper
tags:
  - academic
page(s): ""
---
> [!abstract]
> We study the problem of learning to generate an answer (or completion) to a question (or prompt), where there could be multiple correct answers, any one of which is acceptable at test time. Learning is based on demonstrations of some correct answer to each training question, as in Supervised Fine Tuning (SFT). We formalize the problem as oï¬„ine imitation learning in contextual bandits, with demonstrations from some optimal policy, without explicitly observed rewards. Prior work assumes that the demonstrator belongs to a low-complexity policy class, which motivates maximum likelihood estimation (i.e., log-loss minimization). In contrast, we propose relying only on the reward model (specifying which answers are correct) being in a lowcardinality class, which we argue is a weaker assumption. We show that likelihood maximization methods can fail in this case, and instead devise an alternative novel approach that learns with sample complexity logarithmic in the cardinality of the reward class. Our work motivates looking beyond likelihood maximization when learning from correct demonstrations.

# Annotations
(2/10/2026, 2:50:51 PM)

"for simplicity we take the reward to be binary and assume there is always at least one good answer" ([Joshilearninganswercorrect2025](zotero://select/library/items/2XCXWWTX)) ([pdf, p. 1](zotero://open-pdf/library/items/KWM5P87B?page=1&annotation=EA3VTP5R))([Joshilearninganswercorrect2025](zotero://open-pdf/library/items/KWM5P87B?page=1&annotation=EA3VTP5R)) Seems like an arguable assumption

"In most questionâ€“answering or prompt-response systems, we argue that the true objective is often reward maximization rather than distribution matching." ([Joshilearninganswercorrect2025](zotero://select/library/items/2XCXWWTX)) ([pdf, p. 2](zotero://open-pdf/library/items/KWM5P87B?page=2&annotation=UQLYME55))([Joshilearninganswercorrect2025](zotero://open-pdf/library/items/KWM5P87B?page=2&annotation=UQLYME55)) I wholeheartedly agree. Also a good viewpoint for building any ML system in general

"we propose modeling the rewards instead of the policy class, and relying only on the reward model class having low cardinality (Section 2). That is the function Ïƒâˆ— : X â†’ 2Y defining correct answers, comes  from a low-cardinality model class Ïƒâˆ— âˆˆ S âŠ† (2Y )X and the demonstrator Ï€âˆ— is supported on Ïƒâˆ—." ([Joshilearninganswercorrect2025](zotero://select/library/items/2XCXWWTX)) ([pdf, p. 2](zotero://open-pdf/library/items/KWM5P87B?page=2&annotation=DTHR8A37))

"Definition 1 (Learning from Correct Demonstrations). We say that the reward model class S is learnable from correct demonstrations by a learning rule A : (X Ã— Y)âˆ— â†’ (âˆ†(Y))X with sample complexity mS,A(Ç«, Î´), if for any Îµ, Î´ âˆˆ (0, 1), and any sample size m â‰¥ mS,A(Îµ, Î´), for any D, and Ïƒâˆ— âˆˆ S, and any Ï€âˆ— âˆˆ Î Ïƒâˆ— , we have  PSâˆ¼(DÃ—Ï€âˆ—)m [LD,Ïƒâˆ— (A(S)) â‰¤ Îµ] â‰¥ 1 âˆ’ Î´ ." ([Joshilearninganswercorrect2025](zotero://select/library/items/2XCXWWTX)) ([pdf, p. 4](zotero://open-pdf/library/items/KWM5P87B?page=4&annotation=DQDFIIJ2))

"Given a policy class Î  âŠ† (âˆ†(Y))X , the (conditional) Maximum Likelihood Estimator (MLE) (or the log-loss minimizer) is defined as  MLEÎ (S) = arg max  Ï€âˆˆÎ   âˆm  i=1  Ï€(yi | xi) = arg min  Ï€âˆˆÎ   (  âˆ’  âˆ‘ m  i=1  log Ï€(yi | xi)  )  . (MLE)" ([Joshilearninganswercorrect2025](zotero://select/library/items/2XCXWWTX)) ([pdf, p. 4](zotero://open-pdf/library/items/KWM5P87B?page=4&annotation=SCNJPVPW))

"The loss LD,Ïƒâˆ—(Ì‚Ï€mle) can then be bounded in terms of  this squared Hellinger distance. However, this guarantee is only meaningful when log |Î | is small. Note that if MLE succeeds thanks to Proposition 1 and the cardinality |Î | being small, we would not only have low loss, but Ì‚Ï€mle would also match the distribution of Ï€âˆ—." ([Joshilearninganswercorrect2025](zotero://select/library/items/2XCXWWTX)) ([pdf, p. 5](zotero://open-pdf/library/items/KWM5P87B?page=5&annotation=3CQTCBWX))

"Let us think of how Ì‚Ï€mle âˆˆ MLEÎ S (S) would behave. For any x observed in the training set, any distribution over correct answers is allowed, and only correct answers are observed, so Ì‚Ï€mle(Â· | x) will match the empirical distribution Ï€S(Â·|x) of observed yiâ€™s for xi = x. However, on unseen contexts x, it may choose any distribution over actions y âˆˆ Ïƒ(x) for some Ïƒ that is consistent with the data, i.e., in:  consS (S) = {Ïƒ âˆˆ S | âˆ€(x,y)âˆˆS y âˆˆ Ïƒ(x)} ." ([Joshilearninganswercorrect2025](zotero://select/library/items/2XCXWWTX)) ([pdf, p. 5](zotero://open-pdf/library/items/KWM5P87B?page=5&annotation=PD4I883R))([Joshilearninganswercorrect2025](zotero://open-pdf/library/items/KWM5P87B?page=5&annotation=PD4I883R)) I believe the intuition here is that for the few seen contexts/examples of x, MLE would learn to imitate the demonstrator's behavior. However, for the unseen contexts/examples, the MLE can only try to learn to be consistent with the dataset without much guidance, making this a very difficult problem

"Consider S = {Ïƒ0, Ïƒ01}, where Ïƒ0(x) = {0} and Ïƒ01(x) = {0, 1} for all x. If the true hypothesis is Ïƒâˆ— = Ïƒ0, then all observed labels are 0. However, Ïƒ01 also remains consistent, and thus MLEÎ S (S) may output 1 at test timeâ€”failing to generalize and go beyond memorization." ([Joshilearninganswercorrect2025](zotero://select/library/items/2XCXWWTX)) ([pdf, p. 5](zotero://open-pdf/library/items/KWM5P87B?page=5&annotation=D6NQPB2D))([Joshilearninganswercorrect2025](zotero://open-pdf/library/items/KWM5P87B?page=5&annotation=D6NQPB2D)) Previous point

"We might instead consider using Maximum Likelihood Estimation, but over a smaller policy class whose size matches that of S. A natural choice for such a policy class is:  Î unif,S := {Ï€unif,Ïƒ : Ïƒ âˆˆ S} where Ï€unif,Ïƒ(Â· | x) = Unif(Ïƒ(x)),  where for simplicity we consider Ïƒ(x) to be finite here. The advantage is that we now have |Î unif,S | = |S|. However, this policy class is misspecified in that the actual demonstrator Ï€âˆ— need not be uniform on all correct answers and so Î unif,S might not contain Ï€âˆ—." ([Joshilearninganswercorrect2025](zotero://select/library/items/2XCXWWTX)) ([pdf, p. 6](zotero://open-pdf/library/items/KWM5P87B?page=6&annotation=HBJYFPAN))

"We begin with a simple rule that does ensure learning, but with high (super) linear sample complexity. We then amend it to obtain the desired logarithmic sample complexity." ([Joshilearninganswercorrect2025](zotero://select/library/items/2XCXWWTX)) ([pdf, p. 6](zotero://open-pdf/library/items/KWM5P87B?page=6&annotation=M4ID923P))

![[Attachments/ğŸ’¤Zotero/joshiLearningAnswerCorrect2025/joshiLearningAnswerCorrect2025-8-x65-y452.png]]([Joshilearninganswercorrect2025](zotero://open-pdf/library/items/KWM5P87B?page=8&annotation=L2N9VEF2)) Essentially, I think we are pushing apart wildly differing hypotheses. If a hypothesis was correct in the given round, yet disagreed with the majority, it is weighted much higher, so that its variability won't be hidden in the masses, and can help eliminate uncertainty faster

"As with Ì‚Ï€Maj, we zero out the weights of Ïƒ that are not consistent with the demonstration yt, and thus completely remove them from consideration (at least with Î² = 0, which we use in the realizable setting). The difference is that in addition, we also increase the weights of Ïƒ under which the predicted action Ì‚yt is incorrect (i.e., Ì‚yt âˆˆ/ Ïƒ(xt), and so Ïƒâ€™s vote did not help elect Ì‚yt)." ([Joshilearninganswercorrect2025](zotero://select/library/items/2XCXWWTX)) ([pdf, p. 8](zotero://open-pdf/library/items/KWM5P87B?page=8&annotation=H2KX7XM6))

"We do so despite the fact that we do not know whether Ì‚yt is correct or not." ([Joshilearninganswercorrect2025](zotero://select/library/items/2XCXWWTX)) ([pdf, p. 8](zotero://open-pdf/library/items/KWM5P87B?page=8&annotation=A5P7QSER))

"Now, on any mistakeâ€“meaning Ì‚yt 6âˆˆ Ïƒâˆ—(xt)â€“the weight of Ïƒâˆ— is doubled. If the algorithm made M mistake on a realizable sequence for some Ïƒâˆ— âˆˆ S at the end some t number of rounds, then it must be that  w(t+1)(Ïƒâˆ—) = 2M â‰¤ Wt+1 â‰¤ W1 = |S| , which implies M â‰¤ log2 |S| ." ([Joshilearninganswercorrect2025](zotero://select/library/items/2XCXWWTX)) ([pdf, p. 9](zotero://open-pdf/library/items/KWM5P87B?page=9&annotation=8A9STGBX))([Joshilearninganswercorrect2025](zotero://open-pdf/library/items/KWM5P87B?page=9&annotation=8A9STGBX)) Very weird proof, but kind of makes sense

"Output a randomized predictor as follows:  Ì‚Ï€o2b(Â· | x) = Unif (Ì‚Ï€1(x), . . . , Ì‚Ï€t(x)) := 1  m  m âˆ‘  t=1  Ì‚Ï€t(Â· | x) ." ([Joshilearninganswercorrect2025](zotero://select/library/items/2XCXWWTX)) ([pdf, p. 9](zotero://open-pdf/library/items/KWM5P87B?page=9&annotation=SEE96UEM))([Joshilearninganswercorrect2025](zotero://open-pdf/library/items/KWM5P87B?page=9&annotation=SEE96UEM)) I'm not sure if I'm understanding this correctly, but aren't we essentially doing the Online algorithm, but with each sample i one by one as t? And if this is the case, is obtaining policies for each step t and just combining them to a uniform setting truly a desired behavior? Shouldn't the algorithm have gotten "better" in later steps?

## References
1. [joshiLearningAnswerCorrect2025](zotero://select/library/items/2XCXWWTX)
