---
aliases: Transfer learning of clinical outcomes from preclinical molecular data, principles and perspectives.
authors: Axel Kowald, Israel Barrantes, Steffen Möller, Daniel Palmer, Hugo Murua Escobar, Anne Schwerk, Georg Fuellen
year: 2022
type: paper
tags:
  - academic
  - annotation
page(s):
---
> [!abstract]
> Accurate transfer learning of clinical outcomes from one cellular context to another, between cell types, developmental stages, omics modalities or species, is considered tremendously useful. When transferring a prediction task from a source domain to a target domain, what counts is the high quality of the predictions in the target domain, requiring states or processes common to both the source and the target that can be learned by the predictor reflected by shared denominators. These may form a compendium of knowledge that is learned in the source to enable predictions in the target, usually with few, if any, labeled target training samples to learn from. Transductive transfer learning refers to the learning of the predictor in the source domain, transferring its outcome label calculations to the target domain, considering the same task. Inductive transfer learning considers cases where the target predictor is performing a different yet related task as compared with the source predictor. Often, there is also a need to first map the variables in the input/feature spaces and/or the variables in the output/outcome spaces. We here discuss and juxtapose various recently published transfer learning approaches, specifically designed (or at least adaptable) to predict clinical (human in vivo) outcomes based on preclinical (mostly animal-based) molecular data, towards finding the right tool for a given task, and paving the way for a comprehensive and systematic comparison of the suitability and accuracy of transfer learning of clinical outcomes.

# Annotations  
(9/6/2022, 10:36:40 PM)

“When transferring a prediction task from a source domain to a target domain, what counts is the high quality of the predictions in the target domain, requiring states or processes common to both the source and the target that can be learned by the predictor ref lected by shared denominators.” ([Axel Kowald et al., 2022, p. 1](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=1&annotation=IT35XYRM))

“Transductive transfer learning refers to the learning of the predictor in the source domain, transferring its outcome label calculations to the target domain, considering the same task.” ([Axel Kowald et al., 2022, p. 1](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=1&annotation=ZK4X79TK))

([Axel Kowald et al., 2022, p. 2](zotero://select/library/items/2M9KE4FI)) Real-time clinical tests on living humans can be expensive and extremely limited. Detailed clinical data is difficult to obtain this way.

“While correlative relationships are sometimes sufficient (e.g. for a biomarker to be predictive), causal relationships are telling us much more about the information flow that starts molecularly and ends up in generating the high-level outcome phenotypes that are of ultimate clinical interest [10]. Transfer learning, if based on causal relationships, can thus be expected to be more successful in general.” ([Axel Kowald et al., 2022, p. 2](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=2&annotation=WZ8RHU77))

“As described, clinical molecular data (from human in vivo studies) are scarce mainly due to limited tissue availability and data protection issues, even though blood may be more readily available and genetic information is readily obtainable even though it may not be easy to share.” ([Axel Kowald et al., 2022, p. 2](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=2&annotation=F5TTN3WZ))

“Domain discrepancies can include differences in extracted features, due to divergent biology, e.g. cell types versus tissues, and they can arise from differences in effect measurements, e.g. continuous versus binary outcomes. Therefore, training a computational model on cell lines and testing it on patients violates the i.i.d. assumption that train and test data are from the same distribution [12].” ([Axel Kowald et al., 2022, p. 2](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=2&annotation=VEVDUKBL))

“In their paper, as in the more recent review of Zhuang et al. [14], transfer learning is defined in terms of source and target domains (of features with probability distributions associated with these), as well as source and target tasks (mapping features to labels using predictors) so that the predictor in the target domain is based on training examples from the source domain, which depends on the extraction of shared denominators to enable the transfer into the target domain.” ([Axel Kowald et al., 2022, p. 2](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=2&annotation=YJVULB8B))

“For any real transfer to take place, the source and target domain, or the source and target tasks, must of course be distinct but contain shared denominators to be transferred.” ([Axel Kowald et al., 2022, p. 2](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=2&annotation=GE2WSUYU))

“Considering the transfer gaps just described, a sufficient degree of similarity, or conservation, of the source and the target is the key to accurate transfer learning from a well-sampled source domain to an under-sampled or unknown target domain.” ([Axel Kowald et al., 2022, p. 3](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=3&annotation=NQF5P749))

“If the mapping is sufficiently similar in the two domains, then the predictor can be applied successfully to the target domain. If the shared denominators refer to features, they can be thought of as low-dimensional representations of the input data that reflect the major features for a given prediction task.” ([Axel Kowald et al., 2022, p. 3](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=3&annotation=NG5WI7BH))

“The transductive f lavor of transfer learning (sometimes also called heterogeneous transfer learning) entails different but related source and target domains, but the tasks are the same and the target domain does not need to have any labeled samples [13], so that the predictor that uses the shared denominators from the source domain can be successfully applied to the target domain.” ([Axel Kowald et al., 2022, p. 3](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=3&annotation=LVZDWVE8))

“The inductive f lavor of transfer learning considers tasks that are different yet ‘related’, and the target domain must include labeled samples [13]. While the tasks are different, the source and target domains (and the marginal distributions of the data) are supposed to be the same.” ([Axel Kowald et al., 2022, p. 3](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=3&annotation=8QFQYDH9))

“We also consider the entirely unsupervised flavor of transfer learning, where the source and target domains are different but related and the tasks are also different but related, and none of the samples contain any labels, neither in the source nor in the target domain.” ([Axel Kowald et al., 2022, p. 3](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=3&annotation=D8YFX585))

“As an aside, the notion of pre-training is sometimes used to refer to the first step of transfer learning, as described by [14] for the example of using neural nets to learn images. Pre-training with a large compendium of images then avoids initializing the neural net with random weights, and it prepares for learning a more specific set of images; it can improve accuracy (by avoiding overfitting) as well as execution time of the final classification [16, 17].” ([Axel Kowald et al., 2022, p. 3](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=3&annotation=GEI9BBRR))

“Application areas of transfer learning feature on one hand a target domain where few or no solutions are given, and on the other hand, a source domain where many more solutions are available.” ([Axel Kowald et al., 2022, p. 3](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=3&annotation=83T6A2YH))

“‘Is there a problem domain, and a learning task in that domain, different from what we are looking at, where there are already (many) solutions that may be of relevance to the task we have?’” ([Axel Kowald et al., 2022, p. 3](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=3&annotation=6SKRMKCV))

“‘Is our task a case for inductive, transductive or unsupervised transfer?’” ([Axel Kowald et al., 2022, p. 3](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=3&annotation=NLNLZZTS))

“If the tasks are the same, a transductive method would be sufficient, and ‘domain adaptation’, e.g. by relabeling, may be the way to go; furthermore, no labeled samples are required for the target domain.” ([Axel Kowald et al., 2022, p. 3](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=3&annotation=AKV5YPFD))

“The best result in terms of precision and recall for learning the human labels, and, consequently, differentially expressed genes (DEGs, contrasting ‘healthy’ and ‘sick’), and pathways, were obtained by a semi-supervised neural net, which iteratively used the human data to augment the mouse data sets (when validating the method, the ground truth comes from DEGs and pathways that were identified from human data using human labels).” ([Axel Kowald et al., 2022, p. 4](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=4&annotation=ETFXCBLM))

([Axel Kowald et al., 2022, p. 4](zotero://select/library/items/2M9KE4FI)) (Summary) The most precise method for learning human label predictions (from labeled mouse data) was using a semi-supervised neural net to iteratively "combine" the human data with the mouse data. As an analogy, imagine a model that is really good at guessing if a mouse is sick. You want it to be able to determine if a human is sick. So what you do is you keep the same model except when you are about to test on the human data, you first change them to their mouse "equivalent". That way, a model that is good at predicting on mice will still perform well on your artificial "mice".

“Initially, the neural net classifier was exclusively trained on labeled mouse data and used to predict human labels based on human expression data. In the next step, the human samples with the highest classification confidence were used to generate an augmented training set consisting of mouse and human data. After re-training, the classifier was then anew applied to the remaining human data, and again, the samples with the highest classification confidence were incorporated into the cross-species training set.” ([Axel Kowald et al., 2022, p. 4](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=4&annotation=SV79ABP9))

“Transductive: supervised modeling (mouse) amended iteratively by semi-supervised retraining (adding unlabeled human data); classification task” ([Axel Kowald et al., 2022, p. 5](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=5&annotation=8EEMESGX))

“Inductive: adversarial domain adaptation and multi-task learning (predicting outcomes for both source and target) using deep neural nets; classification task in the target domain” ([Axel Kowald et al., 2022, p. 5](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=5&annotation=DVEZAYTT))

“Note that this algorithm does not require the true human labels, the integration works by only using the predicted labels; the true human labels are used for validation. This strategy is a clever way to humanize animal data and seems to be applicable to a wide field of problems and shows high relevance, given the lack of generalization that is often encountered between mouse and human biology.” ([Axel Kowald et al., 2022, p. 6](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=6&annotation=WPK3QCA8))

“It does, however, require a classification task and is not suitable for regression problems (such as predicting age, speed or other numerical values), since only in the classification case the output of the machine learning algorithm can be used to assign a high or low confidence to the prediction (depending on how much a prediction is ‘between’ classes).” ([Axel Kowald et al., 2022, p. 6](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=6&annotation=AW4BD6NB))

“Firstly, it transfers gene expression knowledge based on cell-lines to patients, where the expression profiling was done to describe the response to chemotherapeutic drugs. For cell lines, the data stem from the GDSC (Genomics of Drug Sensitivity in Cancer) database, and the labels are IC50 values. For patients, the data stem from the TCGA and some other sources, and the labels (if available) are binary, reflecting response/nonresponse to chemotherapy (yes/no). Then, the different output labels are handled by multi-task learning. More specifically, a multi-class predictor is trained on both source and target samples, utilizing a binarized outcome in the case of the source samples; this simultaneous learning on the source and target data is also suggested to improve accuracy. The ‘biological’ differences in the gene expression input data are handled by adversarial domain adaptation. In more detail, shared denominators (called ‘extracted features’ in the AITL framework) are learned in a domain-invariant manner by employing an adversary network tasked with distinguishing the domains; its failure is rewarded. If the extracted features learned by AITL play a similar role in both source and target domains, AITL transfer learning can be successful.” ([Axel Kowald et al., 2022, p. 7](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=7&annotation=6K5WXUIT))

([Axel Kowald et al., 2022, p. 7](zotero://select/library/items/2M9KE4FI)) Labels exits for both the source domain and the target domain. The objective of AITL is to learn how labels in the target domain can be easily calculated. Instead of simply using the target data and labels alone for training (due to a lack of data), a multi-class model is trained on the source domain data at the same time. Common features are learned from this process. It is these features that are then used to train a model further on the source domain, which can then be used for the target domain.

“To identify common molecular mechanisms (based on similarity of gene expression) in preclinical models and human tumors, PRECISE processes transcriptomic data to first find specific underlying ‘factors’ (based on a PCA) for each set (preclinical models and human tumors) separately, and the factors from both sets are then aligned and compared, to generate common factors (or principal vectors) between both sets, the most similar of which are then used to generate a consensus representation of the tumor model. This consensus representation is then finally employed to train a regression model of the preclinical gene expression data with respect to the preclinical drug response data, which is then applied to the real human tumor gene expression data to predict human tumor response.” ([Axel Kowald et al., 2022, p. 8](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=8&annotation=9HCC9TZM))

([Axel Kowald et al., 2022, p. 8](zotero://select/library/items/2M9KE4FI)) To find common molecular factors between the source data and the target data, a technical comparison is made using Principal Vectors (PCA). Once these common factors are determined, a model (using these factors) is trained on the source domain and later used to make predictions for the target domain. This has a similar concept to AITL.

“As the name implies, the architecture of trVAE is based on an autoencoder, an unsupervised neural net where the output layer is trained to reproduce the input layer while going through a bottleneck layer in between. trVAE modifies this approach by explicitly providing the first decoder layer with information about the condition of the input sample (e.g. smiling versus non-smiling). During training, all samples are supplied with their correct condition, but for prediction the desired condition (e.g. smiling) is used as extra input to the decoder, causing the last layer of the decoder to contain a representation of the input modified by the desired condition.” ([Axel Kowald et al., 2022, p. 8](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=8&annotation=GJIGC9Q4))

([Axel Kowald et al., 2022, p. 8](zotero://select/library/items/2M9KE4FI)) This is an unsupervised learning approach using an autoencoder. An autoencoder is an unsupervised neural network that takes an input, passes it through a bottleneck layer, and tries to recreate the input. The same approach is applied in this paper, except with a twist. The autoencoder receives an input but also paired with a condition (like smiling or not smiling). The data passes through the bottleneck layer like usual, but before the model tries to decode it, it needs a condition again. For training, the autoencoder is fed correct pairings for conditions (i.e smiling image + smiling condition -> smiling condition, frowning image + frowning condition -> frowning condition). For testing, however, the condition is switched for a set that has no data (i.e smiling image + smiling condition -> frowning condition). In this way, target data is automatically transferred to a source data representation. Similar to Semisupervised Transfer Learning.

“researchers use alternative model systems in the hope that the insights from those systems can be applied to humans, yet, due to the limited generalizability of such alternative model systems, conclusions drawn can lead to failures. Accurate transfer learning is expected to improve this situation by extracting the shared denominators of the two domains, preclinical and clinical.” ([Axel Kowald et al., 2022, p. 8](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=8&annotation=AYJNYB7D))

“Finally, it is very helpful to transfer knowledge from tissue to tissue. In humans, it is often not possible to obtain a sample from the affected tissue (e.g. brain or pancreas), but a blood sample can be collected easily and non-invasively. Moreover, the f low of blood connects most tissues of the body in one way or another, so we can expect to find traces of many organ-specific processes in the blood.” ([Axel Kowald et al., 2022, p. 10](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=10&annotation=KALNTYUJ))

“For transfer learning from tissue to tissue, trVAE may be employed.” ([Axel Kowald et al., 2022, p. 10](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=10&annotation=ECTPKMHE))

“In vivo: a process or experiment that occurs in a living organism, e.g. clinical intervention trials.” ([Axel Kowald et al., 2022, p. 10](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=10&annotation=KP9VITNP))

“In vitro: a process or experiment that occurs outside a living organism, isolating the sample from its typical biological context, e.g. cell culture experiments using established cell lines.” ([Axel Kowald et al., 2022, p. 10](zotero://select/library/items/2M9KE4FI)) ([pdf](zotero://open-pdf/library/items/56AEN8PB?page=10&annotation=WJMAI37T))

## References
1. [axelkowald.etal_2022](zotero://select/items/@axelkowald.etal_2022)
