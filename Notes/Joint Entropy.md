---
alias: []
subject: Data Science Intro
tags: [undergrad]
---
# Joint Entropy

> [!note]
> A measurement of the entropy of a joint distribution (with multiple variables). Takes the probability of two variables together and multiplies it by the $\log$ of that probability.

> [!math]
> $H(E, C) \equiv H(P(E, C))=-\sum_{e \in \mathcal{E}} \sum_{c \in \mathcal{C}} P(e, c) \log P(e, c)$
> $E, C =$ distributions of possibilities of variables
> $P(e,c)=$ probability of possibility e and c occuring together

## References
1. [[Entropy]]
2. [[Probability]]