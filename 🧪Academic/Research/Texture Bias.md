In a sense, Machine Learning models are extremely lazy. Their only objective is to maximize some measure (usually accuracy) given choices to make based on information, and without any restrictions on *how* they maximize these measures, they often will develop the easiest technique possible. Because of this, simply looking at a model's accuracy score does not tell us how well the model has learned methods the same way we do, and this paper is a prime example of this.
At first glance, state of the art Convolutional Neural Networks (CNNs) seem to be prime examples of human-like thinking written in code. Depending on their data and duration of training, they are nearly 100% accurate in distinguishing and classifying images. However, while these magical black boxes certainly accomplish their tasks, their methodology was somewhat a mystery. With large neural networks taking inputs to functions, passing the outputs to other functions, and calculating the changes that need to be made in each funciton, it is not easy to understand exactly how a model can identify a cat, for example. The work of Geirhos et al. in 2018 made these methodologies much clearer:

> ![[intro_figure.png|center|700]]

Evidently, these CNNs trained to classify images had been accomplishing their tasks by simply looking at the texture of an figure. As Figure 1 (c) shows, even when an image that is obviously shaped like a cat is presented to the model, the model seemingly ignores that in favor of looking at the small patterns of lines and shapes that make up the cat's texture.
While this training may be sufficint in controlled environments with high quality images, the danger may come in applying these models in the real world. Strange lighting,  may affect visibility of specific features,
